{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP38GRaXwucD2R1fCTBBMtF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TashreefMuhammad/Transformer-to-predict-stock-market-using-time2vec/blob/main/Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer-Based Deep Learning Model for Stock Price Prediction: A Case Study on Bangladesh Stock Market\n",
        "\n",
        "This notebook contaiins some of the initial components of the research title that have been given.\n",
        "\n",
        "It is not the final version, but it comprises almost all of the required basic functionalities. It will help readers understand better the formation of the code.\n",
        "\n",
        "This code is provided as a courtesy so that people who find the topic interesting, can get ideas on implementation of the code. Certain parts are hidden from the code as there are further studies that we are running in this area.\n",
        "\n",
        "However, I hope that the code will be capable of running if correct environment is set-up.\n",
        "\n",
        "All associated research related data for this topic is shared following"
      ],
      "metadata": {
        "id": "kpgiWbW5eCal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-print\n",
        "\n",
        "The pre-print of the research was published on [arXiv](https://arxiv.org/). The paper can be found through the identifier [2208.08300](https://arxiv.org/abs/2208.08300). The pre-print does not contain all the details that are available in the published version. The published version is peer-reviewd and far more enhanced and polished with further investigation and assessments.\n",
        "\n",
        "I would heavily encourage you to read the published version if you want to get the full potential of the study."
      ],
      "metadata": {
        "id": "sBOm5fsUztq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Research Article\n",
        "This paper was peer-reviewed and published in [International Journal of Computational Intelligence and Applications](https://www.worldscientific.com/worldscinet/ijcia) also known as IJCIA. Yoou can find the article [here](https://www.worldscientific.com/doi/10.1142/S146902682350013X)."
      ],
      "metadata": {
        "id": "DAaZcxql0dH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "The associated dataset of the conducted research, and researches related on this topic will be published through [Mendeley Data](https://data.mendeley.com/). The dataset can be found using the DOI: [10.17632/23553sm4tn](https://data.mendeley.com/datasets/23553sm4tn)"
      ],
      "metadata": {
        "id": "awK6gngp1Wum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code\n",
        "\n",
        "The remaining part of this notebook contains codes that should work. This can be considered as the official code as it is mostly similar to the ones used for producing the results of this study but has been modified by cleaning the outlook and removing personal informations for being shared on public repository."
      ],
      "metadata": {
        "id": "Pkrnt2i62CJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries\n",
        "\n",
        "Import libraries that will be necessarry for the experiemnt"
      ],
      "metadata": {
        "id": "sZwj2ouwgvCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "print('Tensorflow version: {}'.format(tf.__version__))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "9bisv5HchIvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Declaring Variables and Values\n",
        "\n",
        "Declaring some variables and values that will be used throughout the experiment"
      ],
      "metadata": {
        "id": "rKdUfdvChPEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "seq_len = 8\n",
        "features = 5\n",
        "# I created a specific folder in my Google Drive to store data and to write data\n",
        "# It is up to you, on how to set it up. I did it for easy coding\n",
        "PATHRead = ''\n",
        "PATHSave = ''\n",
        "\n",
        "# The research consentrated on Daily abd Weekly Data. On the paper, it has been\n",
        "# explained on how to gain Weekly data from Daily Data\n",
        "chartType = ['Daily', 'Weekly']\n",
        "# In the conducted study, we worked on eight companies. These are there\n",
        "# Trading Codes\n",
        "companyNames = ['1JANATAMF', 'AAMRANET', 'ABBANK', 'ACI', 'ACIFORMULA', 'AGRANINS', 'ALLTEX', 'DELTALIFE']\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "QkUGhFDUhmAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Transformer Model\n",
        "\n",
        "Build the necessarry classes and instances for ***Transformer Model***. Ideas for this part was taken after reviewing codes from [JanSchm/CapMarket](https://github.com/JanSchm/CapMarket) where he used the *time2vec* and developed the backbone of the defined model."
      ],
      "metadata": {
        "id": "_m2di33PAFMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Hyperparameters\n",
        "\n",
        "Define hyperparameter for later use"
      ],
      "metadata": {
        "id": "Umod-qZzAg9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = 256\n",
        "d_v = 256\n",
        "n_heads = 12\n",
        "ff_dim = 256"
      ],
      "metadata": {
        "id": "4jrLzHvBApjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Time2Vec\n",
        "\n",
        "Implementation of time2vec"
      ],
      "metadata": {
        "id": "DUlHmW6-A2wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Time2Vector(Layer):\n",
        "  def __init__(self, seq_len, **kwargs):\n",
        "    super(Time2Vector, self).__init__()\n",
        "    self.seq_len = seq_len\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    '''Initialize weights and biases with shape (batch, seq_len)'''\n",
        "    self.weights_linear = self.add_weight(name='weight_linear',\n",
        "                                shape=(int(self.seq_len),),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "\n",
        "    self.bias_linear = self.add_weight(name='bias_linear',\n",
        "                                shape=(int(self.seq_len),),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "\n",
        "    self.weights_periodic = self.add_weight(name='weight_periodic',\n",
        "                                shape=(int(self.seq_len),),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "\n",
        "    self.bias_periodic = self.add_weight(name='bias_periodic',\n",
        "                                shape=(int(self.seq_len),),\n",
        "                                initializer='uniform',\n",
        "                                trainable=True)\n",
        "\n",
        "  def call(self, x):\n",
        "    '''Calculate linear and periodic time features'''\n",
        "    x = tf.math.reduce_mean(x[:,:,:4], axis=-1)\n",
        "    time_linear = self.weights_linear * x + self.bias_linear\n",
        "    time_linear = tf.expand_dims(time_linear, axis=-1)\n",
        "\n",
        "    time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
        "    time_periodic = tf.expand_dims(time_periodic, axis=-1)\n",
        "    return tf.concat([time_linear, time_periodic], axis=-1)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "    config.update({'seq_len': self.seq_len})\n",
        "    return config"
      ],
      "metadata": {
        "id": "Pay_k-sXCJfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Layers\n",
        "\n",
        "Make attention layers"
      ],
      "metadata": {
        "id": "8ZJ4YDTdCPG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleAttention(Layer):\n",
        "  def __init__(self, d_k, d_v):\n",
        "    super(SingleAttention, self).__init__()\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.query = Dense(self.d_k,\n",
        "                       input_shape=input_shape,\n",
        "                       kernel_initializer='glorot_uniform',\n",
        "                       bias_initializer='glorot_uniform')\n",
        "\n",
        "    self.key = Dense(self.d_k,\n",
        "                     input_shape=input_shape,\n",
        "                     kernel_initializer='glorot_uniform',\n",
        "                     bias_initializer='glorot_uniform')\n",
        "\n",
        "    self.value = Dense(self.d_v,\n",
        "                       input_shape=input_shape,\n",
        "                       kernel_initializer='glorot_uniform',\n",
        "                       bias_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    q = self.query(inputs[0])\n",
        "    k = self.key(inputs[1])\n",
        "\n",
        "    attn_weights = tf.matmul(q, k, transpose_b=True)\n",
        "    attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
        "    attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
        "\n",
        "    v = self.value(inputs[2])\n",
        "    attn_out = tf.matmul(attn_weights, v)\n",
        "    return attn_out\n",
        "\n",
        "#############################################################################\n",
        "\n",
        "class MultiAttention(Layer):\n",
        "  def __init__(self, d_k, d_v, n_heads):\n",
        "    super(MultiAttention, self).__init__()\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.n_heads = n_heads\n",
        "    self.attn_heads = list()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    for n in range(self.n_heads):\n",
        "      self.attn_heads.append(SingleAttention(self.d_k, self.d_v))\n",
        "\n",
        "    self.linear = Dense(input_shape[0][-1],\n",
        "                        input_shape=input_shape,\n",
        "                        kernel_initializer='glorot_uniform',\n",
        "                        bias_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n",
        "    concat_attn = tf.concat(attn, axis=-1)\n",
        "    multi_linear = self.linear(concat_attn)\n",
        "    return multi_linear\n",
        "\n",
        "#############################################################################\n",
        "\n",
        "class TransformerEncoder(Layer):\n",
        "  def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.n_heads = n_heads\n",
        "    self.ff_dim = ff_dim\n",
        "    self.attn_heads = list()\n",
        "    self.dropout_rate = dropout\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
        "    self.attn_dropout = Dropout(self.dropout_rate)\n",
        "    self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
        "\n",
        "    self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n",
        "    self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1)\n",
        "    self.ff_dropout = Dropout(self.dropout_rate)\n",
        "    self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    attn_layer = self.attn_multi(inputs)\n",
        "    attn_layer = self.attn_dropout(attn_layer)\n",
        "    attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
        "\n",
        "    ff_layer = self.ff_conv1D_1(attn_layer)\n",
        "    ff_layer = self.ff_conv1D_2(ff_layer)\n",
        "    ff_layer = self.ff_dropout(ff_layer)\n",
        "    ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
        "    return ff_layer\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "    config.update({'d_k': self.d_k,\n",
        "                   'd_v': self.d_v,\n",
        "                   'n_heads': self.n_heads,\n",
        "                   'ff_dim': self.ff_dim,\n",
        "                   'attn_heads': self.attn_heads,\n",
        "                   'dropout_rate': self.dropout_rate})\n",
        "    return config"
      ],
      "metadata": {
        "id": "fxEU_sT5Cdm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Transformer Model\n",
        "\n",
        "Create instance of ***Transformer Model*** using the following method\n",
        "\n",
        "One think I would like to point out, is that in the research, RMSE was used. But as you can see that we are using Tensorflow and Keras, they do not provide an easy option for using RMSE. Hence, I took idea from [StackOverflow](https://stackoverflow.com/questions/43855162/rmse-rmsle-loss-function-in-keras) question answers and implemented RMSE as the loss function."
      ],
      "metadata": {
        "id": "zjYq_iznCi6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(y_true, y_pred):\n",
        "  return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
        "\n",
        "def createTransformerModel():\n",
        "  # Initialize time and transformer layers\n",
        "  time_embedding = Time2Vector(seq_len)\n",
        "  attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "  attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "  attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
        "\n",
        "  # Construct model\n",
        "  in_seq = Input(shape=(seq_len, features))\n",
        "  x = time_embedding(in_seq)\n",
        "  x = Concatenate(axis=-1)([in_seq, x])\n",
        "  x = attn_layer1((x, x, x))\n",
        "  x = attn_layer2((x, x, x))\n",
        "  x = attn_layer3((x, x, x))\n",
        "  x = GlobalAveragePooling1D(data_format='channels_first')(x)\n",
        "  x = Dropout(0.1)(x)\n",
        "  x = Dense(64, activation='relu')(x)\n",
        "  x = Dropout(0.1)(x)\n",
        "  out = Dense(1, activation='linear')(x)\n",
        "\n",
        "  model = Model(inputs=in_seq, outputs=out)\n",
        "  model.compile(loss=loss_fn, optimizer='adam', metrics=['mae'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "1HnBSvaKCssB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print Developed Model\n",
        "\n",
        "Print the developed model"
      ],
      "metadata": {
        "id": "-S8_qpdDCwHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def printModelStructure(model, path):\n",
        "  tf.keras.utils.plot_model(\n",
        "      model,\n",
        "      to_file = path,\n",
        "      show_shapes = True,\n",
        "      show_layer_names = True,\n",
        "      expand_nested = True,\n",
        "      dpi = 96,)\n",
        "\n",
        "printModelStructure(createTransformerModel(), '{}TransformerModelArchitecture.png'.format(PATHSave))"
      ],
      "metadata": {
        "id": "2UnbhUG9C0vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Manipulation\n",
        "\n",
        "Converting Data to a useful format"
      ],
      "metadata": {
        "id": "TsLlkgIIKrhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract CSV File\n",
        "\n",
        "Extract data from CSV file"
      ],
      "metadata": {
        "id": "jslvrq4iKyCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(path_data):\n",
        "  df = pd.read_csv(path_data)\n",
        "\n",
        "  # Formating the Data column as Bangladesh Standard System (d-m-y) is not directly interpreted in python\n",
        "  df['Date'] = pd.to_datetime(df['Date'], dayfirst = True)\n",
        "\n",
        "  # Replace 0 to avoid dividing by 0 later on\n",
        "  df['Volume'].replace(to_replace=0, method='ffill', inplace=True)\n",
        "\n",
        "  # Sorting by Date just to ensure dataframe is sorted by date\n",
        "  df.sort_values(by = 'Date', inplace = True, ignore_index = True)\n",
        "\n",
        "  # Apply moving average with a window of 10 days to all columns\n",
        "  df[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].rolling(10).mean()\n",
        "\n",
        "\n",
        "  # Drop all rows with NaN values\n",
        "  df.dropna(how='any', axis=0, inplace=True)\n",
        "  df.reset_index(drop=True)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "xr9_rf0QK7tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stationize and Normalize Data\n",
        "\n",
        "Making data stationary and normalizing it"
      ],
      "metadata": {
        "id": "gnq2R3JWK_4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizeData(df):\n",
        "  ###################################\n",
        "  #== Calculate percentage change ==#\n",
        "  ###################################\n",
        "\n",
        "  df['Open'] = df['Open'].pct_change() # Create arithmetic returns column\n",
        "  df['High'] = df['High'].pct_change() # Create arithmetic returns column\n",
        "  df['Low'] = df['Low'].pct_change() # Create arithmetic returns column\n",
        "  df['Close'] = df['Close'].pct_change() # Create arithmetic returns column\n",
        "  df['Volume'] = df['Volume'].pct_change()\n",
        "\n",
        "  df.dropna(how='any', axis=0, inplace=True) # Drop all rows with NaN values\n",
        "  # print(df)\n",
        "\n",
        "  ##################################\n",
        "  #== Find Indices for Splitting ==#\n",
        "  ##################################\n",
        "\n",
        "  times = sorted(df.index.values)\n",
        "  last_10pct = sorted(df.index.values)[-int(0.1*len(times))] # Last 10% of series\n",
        "  last_20pct = sorted(df.index.values)[-int(0.2*len(times))] # Last 20% of series\n",
        "\n",
        "  ###############################################################################\n",
        "\n",
        "  ###############################\n",
        "  #== Normalize price columns ==#\n",
        "  ###############################\n",
        "\n",
        "  min_return = min(df[(df.index < last_20pct)][['Open', 'High', 'Low', 'Close']].min(axis=0))\n",
        "  max_return = max(df[(df.index < last_20pct)][['Open', 'High', 'Low', 'Close']].max(axis=0))\n",
        "\n",
        "  # Min-max normalize price columns (0-1 range)\n",
        "  df['Open'] = (df['Open'] - min_return) / (max_return - min_return)\n",
        "  df['High'] = (df['High'] - min_return) / (max_return - min_return)\n",
        "  df['Low'] = (df['Low'] - min_return) / (max_return - min_return)\n",
        "  df['Close'] = (df['Close'] - min_return) / (max_return - min_return)\n",
        "\n",
        "  ###############################################################################\n",
        "\n",
        "  ###############################\n",
        "  #== Normalize volume column ==#\n",
        "  ###############################\n",
        "\n",
        "  min_volume = df[(df.index < last_20pct)]['Volume'].min(axis=0)\n",
        "  max_volume = df[(df.index < last_20pct)]['Volume'].max(axis=0)\n",
        "\n",
        "  # Min-max normalize volume columns (0-1 range)\n",
        "  df['Volume'] = (df['Volume'] - min_volume) / (max_volume - min_volume)\n",
        "\n",
        "  ###############################################################################\n",
        "\n",
        "  ##################################################\n",
        "  #== Create training, validation and test split ==#\n",
        "  ##################################################\n",
        "\n",
        "  df_train = df[(df.index < last_20pct)]  # Training data are 80% of total data\n",
        "  df_val = df[(df.index >= last_20pct) & (df.index < last_10pct)]\n",
        "  df_test = df[(df.index >= last_10pct)]\n",
        "\n",
        "  # Remove date column\n",
        "  df_train.drop(columns=['Date'], inplace=True)\n",
        "  df_val.drop(columns=['Date'], inplace=True)\n",
        "  df_test.drop(columns=['Date'], inplace=True)\n",
        "\n",
        "  # Convert pandas columns into arrays\n",
        "  train_data = df_train.values\n",
        "  val_data = df_val.values\n",
        "  test_data = df_test.values\n",
        "\n",
        "  print('Training data shape: {}'.format(train_data.shape))\n",
        "  print('Validation data shape: {}'.format(val_data.shape))\n",
        "  print('Test data shape: {}'.format(test_data.shape))\n",
        "\n",
        "  # df_train.head()\n",
        "  return df, train_data, val_data, test_data, df_train, df_val, df_test\n",
        "\n",
        "def unSplittedNormalize(df):\n",
        "  tmp_data = df.copy()\n",
        "  ###################################\n",
        "  #== Calculate percentage change ==#\n",
        "  ###################################\n",
        "\n",
        "  df['Open'] = df['Open'].pct_change() # Create arithmetic returns column\n",
        "  df['High'] = df['High'].pct_change() # Create arithmetic returns column\n",
        "  df['Low'] = df['Low'].pct_change() # Create arithmetic returns column\n",
        "  df['Close'] = df['Close'].pct_change() # Create arithmetic returns column\n",
        "  df['Volume'] = df['Volume'].pct_change()\n",
        "\n",
        "  df.dropna(how='any', axis=0, inplace=True) # Drop all rows with NaN values\n",
        "  # print(df)\n",
        "  raw_data = tmp_data[tmp_data['Date'].isin(df['Date'])]\n",
        "\n",
        "\n",
        "  ##################################\n",
        "  #== Find Indices for Splitting ==#\n",
        "  ##################################\n",
        "\n",
        "  times = sorted(df.index.values)\n",
        "  last_10pct = sorted(df.index.values)[-int(0.1*len(times))] # Last 10% of series\n",
        "  last_20pct = sorted(df.index.values)[-int(0.2*len(times))] # Last 20% of series\n",
        "\n",
        "  ###############################################################################\n",
        "\n",
        "  ###############################\n",
        "  #== Normalize price columns ==#\n",
        "  ###############################\n",
        "\n",
        "  min_return = min(df[(df.index < last_20pct)][['Open', 'High', 'Low', 'Close']].min(axis=0))\n",
        "  max_return = max(df[(df.index < last_20pct)][['Open', 'High', 'Low', 'Close']].max(axis=0))\n",
        "\n",
        "  # Min-max normalize price columns (0-1 range)\n",
        "  df['Open'] = (df['Open'] - min_return) / (max_return - min_return)\n",
        "  df['High'] = (df['High'] - min_return) / (max_return - min_return)\n",
        "  df['Low'] = (df['Low'] - min_return) / (max_return - min_return)\n",
        "  df['Close'] = (df['Close'] - min_return) / (max_return - min_return)\n",
        "\n",
        "  ###############################################################################\n",
        "\n",
        "  ###############################\n",
        "  #== Normalize volume column ==#\n",
        "  ###############################\n",
        "\n",
        "  min_volume = df[(df.index < last_20pct)]['Volume'].min(axis=0)\n",
        "  max_volume = df[(df.index < last_20pct)]['Volume'].max(axis=0)\n",
        "\n",
        "  # Min-max normalize volume columns (0-1 range)\n",
        "  df['Volume'] = (df['Volume'] - min_volume) / (max_volume - min_volume)\n",
        "\n",
        "  ###############################################################################\n",
        "\n",
        "  ##################################################\n",
        "  #== Create training, validation and test split ==#\n",
        "  ##################################################\n",
        "\n",
        "\n",
        "  # Remove date column\n",
        "  df.drop(columns=['Date'], inplace=True)\n",
        "  raw_data.drop(columns=['Date'], inplace=True)\n",
        "\n",
        "  # Convert pandas columns into arrays\n",
        "  data = df.values\n",
        "  raw_data = raw_data.values\n",
        "\n",
        "  print('Data shape: {}'.format(data.shape))\n",
        "\n",
        "  # df_train.head()\n",
        "  return df, raw_data, data"
      ],
      "metadata": {
        "id": "cTq8iWcJquKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting Data\n",
        "\n",
        "Splitting dataset into 8:1:1"
      ],
      "metadata": {
        "id": "qMuoptJ-qwrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def splitData(train_data, val_data, test_data, day_to_predict):\n",
        "  # Training data\n",
        "  day_to_predict -= 1\n",
        "  X_train, y_train = [], []\n",
        "  for i in range(seq_len, len(train_data) - day_to_predict):\n",
        "    X_train.append(train_data[i-seq_len:i])               # Chunks of training data with a length of seq_len df-rows\n",
        "    y_train.append(train_data[:, 3][i + day_to_predict])  # Value of 4th column (Close Price) of df-row seq_len\n",
        "  X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "  ###############################################################################\n",
        "\n",
        "  # Validation data\n",
        "  X_val, y_val = [], []\n",
        "  for i in range(seq_len, len(val_data) - day_to_predict):\n",
        "    X_val.append(val_data[i-seq_len:i])\n",
        "    y_val.append(val_data[:, 3][i + day_to_predict])\n",
        "  X_val, y_val = np.array(X_val), np.array(y_val)\n",
        "\n",
        "  ###############################################################################\n",
        "\n",
        "  # Test data\n",
        "  X_test, y_test = [], []\n",
        "  for i in range(seq_len, len(test_data) - day_to_predict):\n",
        "    X_test.append(test_data[i-seq_len:i])\n",
        "    y_test.append(test_data[:, 3][i + day_to_predict])\n",
        "  X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "\n",
        "  return X_train, y_train, X_val, y_val, X_test, y_test"
      ],
      "metadata": {
        "id": "xmcJ8gAGq3zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting Data\n",
        "\n",
        "Plot data in different parts of training and evaluating model"
      ],
      "metadata": {
        "id": "7r_1WIULAZ-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotRawData(df, name, path):\n",
        "  fig = plt.figure(figsize=(15,5))\n",
        "  st = fig.suptitle('Real Closing Price of {}'.format(name), fontsize=20)\n",
        "\n",
        "  ax1 = fig.add_subplot(111)\n",
        "  ax1.plot(df['Close'], label = name + ' Close Price')\n",
        "  ax1.set_xticks(range(0, df.shape[0], df.shape[0] // 5))\n",
        "  ax1.set_xticklabels(df['Date'].loc[::(df.shape[0] // 5)])\n",
        "  ax1.set_ylabel('Close Price', fontsize = 18)\n",
        "  ax1.legend(loc = 'upper left', fontsize = 12)\n",
        "\n",
        "  plt.savefig(path)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plotNormalizedData(df, name, train_data, val_data, test_data, df_train, df_val, df_test, path):\n",
        "  fig = plt.figure(figsize = (15,6))\n",
        "  st = fig.suptitle('Stationization, Normalization and Data Separation for ' + name, fontsize=20)\n",
        "\n",
        "  ax1 = fig.add_subplot(111)\n",
        "  ax1.plot(np.arange(train_data.shape[0]), df_train['Close'], label = 'Training data')\n",
        "\n",
        "  ax1.plot(np.arange(train_data.shape[0],\n",
        "                     train_data.shape[0] + val_data.shape[0]), df_val['Close'], label = 'Validation data')\n",
        "\n",
        "  ax1.plot(np.arange(train_data.shape[0] + val_data.shape[0],\n",
        "                     train_data.shape[0] + val_data.shape[0] + test_data.shape[0]), df_test['Close'], label = 'Test data')\n",
        "  ax1.set_xlabel('Date')\n",
        "  ax1.set_ylabel('Normalized Closing Returns')\n",
        "  ax1.legend(loc = 'best', fontsize = 12)\n",
        "\n",
        "  plt.savefig(path)\n",
        "  plt.show()\n",
        "\n",
        "def plotPrediction(train_data, val_data, test_data, train_pred, val_pred, test_pred, name, model_Type, path):\n",
        "  #######################\n",
        "  #== Display results ==#\n",
        "  #######################\n",
        "\n",
        "  fig = plt.figure(figsize = (15,20))\n",
        "\n",
        "  # Plot training data results\n",
        "  ax11 = fig.add_subplot(311)\n",
        "  ax11.plot(train_data[:, 3], label = name + ' Closing Returns')\n",
        "  ax11.plot(np.arange(seq_len, train_pred.shape[0]+seq_len), train_pred, linewidth=3, label = 'Predicted ' + name + ' Closing Returns')\n",
        "  ax11.set_title('Training Data', fontsize = 18)\n",
        "  ax11.set_xlabel('Date')\n",
        "  ax11.set_ylabel(name + ' Closing Returns')\n",
        "  ax11.legend(loc = 'best', fontsize = 12)\n",
        "\n",
        "  # Plot validation data results\n",
        "  ax21 = fig.add_subplot(312)\n",
        "  ax21.plot(val_data[:, 3], label = name + ' Closing Returns')\n",
        "  ax21.plot(np.arange(seq_len, val_pred.shape[0]+seq_len), val_pred, linewidth=3, label = 'Predicted ' + name + ' Closing Returns')\n",
        "  ax21.set_title('Validation Data', fontsize = 18)\n",
        "  ax21.set_xlabel('Date')\n",
        "  ax21.set_ylabel(name + ' Closing Returns')\n",
        "  ax21.legend(loc = 'best', fontsize = 12)\n",
        "\n",
        "  # Plot test data results\n",
        "  ax31 = fig.add_subplot(313)\n",
        "  ax31.plot(test_data[:, 3], label = name + ' Closing Returns')\n",
        "  ax31.plot(np.arange(seq_len, test_pred.shape[0]+seq_len), test_pred, linewidth=3, label = 'Predicted ' + name + ' Closing Returns')\n",
        "  ax31.set_title('Test Data', fontsize = 18)\n",
        "  ax31.set_xlabel('Date')\n",
        "  ax31.set_ylabel(name + ' Closing Returns')\n",
        "  ax31.legend(loc = 'best', fontsize = 12)\n",
        "\n",
        "  plt.savefig(path)\n",
        "  plt.show()\n",
        "\n",
        "def plotErrorTraining(history, model_type, name, path):\n",
        "  #############################\n",
        "  #== Display model metrics ==#\n",
        "  #############################\n",
        "\n",
        "  fig = plt.figure(figsize=(15,15))\n",
        "  st = fig.suptitle('Performance Metrics for ' + name + ' Using ' + num2words(features) + ' features', fontsize = 22)\n",
        "  st.set_y(0.92)\n",
        "\n",
        "  # Plot model loss\n",
        "  ax1 = fig.add_subplot(211)\n",
        "  ax1.plot(history.history['loss'], label = 'Training loss (RMSE)')\n",
        "  ax1.plot(history.history['val_loss'], label = 'Validation loss (RMSE)')\n",
        "  ax1.set_title('Root Mean Squared Error (RMSE)', fontsize = 18)\n",
        "  ax1.set_xlabel('Epoch')\n",
        "  ax1.set_ylabel('Loss (RMSE)')\n",
        "  ax1.legend(loc = 'best', fontsize = 12)\n",
        "\n",
        "  # Plot MAE\n",
        "  ax2 = fig.add_subplot(212)\n",
        "  ax2.plot(history.history['mae'], label = 'Training MAE')\n",
        "  ax2.plot(history.history['val_mae'], label = 'Validation MAE')\n",
        "  ax2.set_title('Mean Average Error (MAE)', fontsize = 18)\n",
        "  ax2.set_xlabel('Epoch')\n",
        "  ax2.set_ylabel('Mean average error (MAE)')\n",
        "  ax2.legend(loc = 'best', fontsize = 12)\n",
        "\n",
        "  plt.savefig(path)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "iour60vcAgPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Model\n",
        "\n",
        "Training the Transformer Model"
      ],
      "metadata": {
        "id": "UI361mDInoWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name in companyNames:\n",
        "  for cType in chartType:\n",
        "    PATHExtension = '{}/{}/'.format(cType, name)\n",
        "\n",
        "    # Path to main data\n",
        "    data_path = PATHRead + cType + '_' + name + '.csv'\n",
        "\n",
        "    # Retrieve data to DataFrame\n",
        "    data = prepareData(data_path)\n",
        "    plotRawData(data, name, PATHSave + PATHExtension + '/Raw_Data.png')\n",
        "\n",
        "    # Normalize Data\n",
        "    data, train_data, val_data, test_data, data_train, data_val, data_test = normalizeData(data)\n",
        "    plotNormalizedData(data, name, train_data, val_data, test_data, data_train, data_val, data_test, PATHSave +  PATHExtension + '/Processed.png')\n",
        "\n",
        "    # Split Data\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = splitData(train_data, val_data, test_data, 1)\n",
        "\n",
        "    ##############################\n",
        "    # Create the Transformer Model\n",
        "    ##############################\n",
        "    modelTransformer = createTransformerModel()\n",
        "    modelTransformer.summary()\n",
        "    callbackTransformer = tf.keras.callbacks.ModelCheckpoint(PATHSave +  PATHExtension + '/' + name + '_' + str(features) + '.hdf5',\n",
        "                                                            monitor = 'val_loss',\n",
        "                                                            save_best_only = True,\n",
        "                                                            verbose = 1)\n",
        "    historyTransformer = modelTransformer.fit(X_train, y_train,\n",
        "                                              batch_size = batch_size,\n",
        "                                              epochs = 50,\n",
        "                                              callbacks = [callbackTransformer],\n",
        "                                              validation_data = (X_val, y_val))\n",
        "    # Calculate predication for training, validation and test data and Display\n",
        "    train_pred = modelTransformer.predict(X_train)\n",
        "    val_pred = modelTransformer.predict(X_val)\n",
        "    test_pred = modelTransformer.predict(X_test)\n",
        "    plotPrediction(train_data, val_data, test_data, train_pred, val_pred, test_pred, name, 'Transformer',\n",
        "                   PATHSave +  PATHExtension + '/StationaryPrediction_' + str(features) + '.png')\n",
        "\n",
        "    # Print evaluation metrics for train, val, test\n",
        "    train_eval = modelTransformer.evaluate(X_train, y_train, verbose=0)\n",
        "    val_eval = modelTransformer.evaluate(X_val, y_val, verbose=0)\n",
        "    test_eval = modelTransformer.evaluate(X_test, y_test, verbose=0)\n",
        "    print()\n",
        "    print('Evaluation metrics for ' + name + ' using ' + str(features) + ' features After Training')\n",
        "    print('Training Data - Loss (RMSE): {:.4f}, MAE: {:.4f}'.format(train_eval[0], train_eval[1]))\n",
        "    print('Validation Data - Loss (RMSE): {:.4f}, MAE: {:.4f}'.format(val_eval[0], val_eval[1]))\n",
        "    print('Test Data - Loss (RMSE): {:.4f}, MAE: {:.4f}'.format(test_eval[0], test_eval[1]))\n",
        "    dataDict = {'Error': ['Training RMSE', 'Training MAE', 'Validation RMSE', 'Validation MAE', 'Testing RMSE', 'Testing MAE'],\n",
        "                'Value': [train_eval[0], train_eval[1], val_eval[0], val_eval[1], test_eval[0], test_eval[1]]}\n",
        "    dFrame = pd.DataFrame(dataDict)\n",
        "    dFrame.to_csv('{}{}ErrorValues.csv'.format(PATHSave, PATHExtension), index = False)\n",
        "    plotErrorTraining(historyTransformer, 'Transformer', name, PATHSave +  PATHExtension + '/Error_' + str(features) + '.png')"
      ],
      "metadata": {
        "id": "d_UaiYBansvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Real Closing Price\n",
        "\n",
        "Predicting the real closing price using developed model"
      ],
      "metadata": {
        "id": "hLILRiDeqpLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "errorData = pd.DataFrame(columns = ['TradingCode', 'ChartType', 'RMSE', 'MAE'])\n",
        "\n",
        "for name in companyNames:\n",
        "  for cType in chartType:\n",
        "\n",
        "    PATHExtension = '{}/{}/'.format(cType, name)\n",
        "    data_list = [name, cType]\n",
        "\n",
        "    # Path to main data\n",
        "    data_path = PATHRead + cType + '_' + name + '.csv'\n",
        "    # Retrieve data to DataFrame\n",
        "    data = prepareData(data_path)\n",
        "    # Normalize\n",
        "    data, raw_data, val = unSplittedNormalize(data)\n",
        "\n",
        "    X, y = [], []\n",
        "    for i1 in range(seq_len, len(val)):\n",
        "      X.append(val[i1-seq_len:i1])\n",
        "      y.append(val[:, 3][i1])\n",
        "    X, y = np.array(X), np.array(y)\n",
        "\n",
        "    model = tf.keras.models.load_model(PATHSave +  PATHExtension + name + '_' + str(features) + '.hdf5',\n",
        "                                        custom_objects={'Time2Vector': Time2Vector,\n",
        "                                                        'SingleAttention': SingleAttention,\n",
        "                                                        'MultiAttention': MultiAttention,\n",
        "                                                        'TransformerEncoder': TransformerEncoder,\n",
        "                                                        'loss_fn': loss_fn})\n",
        "    raw_pred = model.predict(X)\n",
        "    raw_eval = model.evaluate(X, y, verbose = 0)\n",
        "\n",
        "    print('Evaluation metrics for ' + name + ' using ' + str(features) + ' features On Full Data')\n",
        "    print('Loss (RMSE): {:.4f}, MAE: {:.4f}'.format(raw_eval[0], raw_eval[1]))\n",
        "    data_list.append(raw_eval[0])\n",
        "    data_list.append(raw_eval[1])\n",
        "    for i1 in range(len(raw_pred)):\n",
        "      raw_pred[i1][0] += raw_data[seq_len + i1][3]\n",
        "\n",
        "    #######################\n",
        "    #== Display results ==#\n",
        "    #######################\n",
        "\n",
        "    plt.figure(figsize = (15,5))\n",
        "    plt.title('Actual Closing Price Prediction for ' + name + ' Using Transformer Model with ' + num2words(features) + ' features', fontsize = 22)\n",
        "    plt.plot(raw_data[:, 3], linewidth = 1, label = name + ' Real Closing Price')\n",
        "    plt.plot(np.arange(seq_len, raw_pred.shape[0]+seq_len), raw_pred, linewidth = 2, color = 'red', label = 'Predicted ' + name + ' Closing Price')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(name + ' Closing Price')\n",
        "    plt.legend(loc = 'best', fontsize = 12)\n",
        "\n",
        "    plt.savefig(PATHSave +  PATHExtension + name + '.png')\n",
        "    plt.show()\n",
        "    errorData.loc[len(errorData)] = data_list\n",
        "errorData.to_excel(PATHSave + 'Errors.xlsx', index = False)"
      ],
      "metadata": {
        "id": "kkcopnAhqwvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>==The End==</center>"
      ],
      "metadata": {
        "id": "jcH_WMInyZH1"
      }
    }
  ]
}